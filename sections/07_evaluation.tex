\chapter{Future Plans}\label{cha:plans}
%\begin{preprompt}
%Lays out your plans for the main tests that you will perform with your model, along with possible model variants or extensions that you might want to investigate (given enough time). It should also sketch a basic timeline for your writing and coding activities during the upcoming 5--6 months of your master work.
%\end{preprompt}

With the completion of this thesis project, we have established how our system fits in with the surrounding research, and in what ways our work could add to this field.
However, before we are able to produce any results, there are three major checkpoints that need to be completed.
Firstly, we must finalize the model, meaning it must be able to run experiments, and validate the results.
The learning issues we have discovered also need to be addressed, such that the baseline model actually converges to an acceptable solution.
Secondly, we must prepare a new dataset which has both bounding boxes and class labels.
Lastly, we must design the exact specifications of each experiment.


\section{Model}
The overview of the development, as described in Section~\ref{sec:model}, provides the outline for what remains to be done in the next stage of the project regarding the system.
We estimate that the majority of the total time needed to develop the system has been expended, and that the time remaining to finalise is in the order of weeks.

When completed the system will take in a labelled train/val dataset and run a training session with either early stopping or for a set number of epochs, then it will run evaluation and the validation set, calculating precision, recall.

Given that the model we are using is many years old at this point, the baseline we establish will most like utilize newer features, such as batch normalization.
Establishing the baseline model will therefore also offer a chance to compare different techniques that have not been tested in this domain before.

\section{Data}
The process of acquiring a new dataset has already begun, but a thorough discussion on the process is left to the next stage of the project.
The data has been provided by the The Norwegian Asthma and Allergy Association (NAAF) and consists of slides of pollen acquired from pollen monitoring stations. 

Pollen is collected with traps where air is continually sucked though a small slit and over an adhesive strip.
The strip is moved across the slit, exposing different sections throughout the day.
Pollen grains and other air born particulates adhere to the strip, which is then analysed under a microscope.
This produces an estimate for the density of different pollen types, measured in particles per cubic meter, throughout the season. 

The dataset used three microscope slides which were photographed using LM imaging.
The total amount of images and labels is unknown as of writing.
Due to the varying flowering seasons of the various species that are analysed, only 2--3 different classes of pollen are observed on any one slide, the total number of classes in the dataset most likely be in the range 3--5.
This dataset is also easily extended, given the fact that historic data from multiple collection stations over multiple seasons exist.
If the developed system were to produce good results, this process could also be accelerated in future iterations.

After the data is collected and labelled it must be partitioned into a training set and testing set.
This process should be semi random, but it is important to balance the datasets in such a way that the distribution of classes is equal.

\section{Experiments}
There are three main experiments that must be designed and executed.
Firstly, various configurations of models must be tested to establish a well performing baseline model.
Secondly the model must be simplified in different ways so that the impact of the changes can be measured.
Third we must incorporate multifocal data into either the training or inference operation and measure the impact.

\subsection{Baseline}
As mentioned, newer techniques have been used in other object detection frameworks that have been published after SSD\@.
These relate to e.g.\ normalization, activation functions, parameter initialization, and bounding box regression parameters.
Of these most are available in PyTorch and are almost trivial to add to the basic SSD\@.
Training times for the prototype model have been on the order of hours, so testing a variety of techniques.
Tuning the hyperparameters for the optimizer also falls under this category of experiments.

\subsection{Simplification}
There are three main ways to simplify the SSD model.
Firstly, we can remove feature layers from the auxiliary structure, thereby limiting the range of scales of the default boxes.
This can by visually explained as removing blocks from Figure~\ref{fig:model}.
Secondly, we can reduce the number of default boxes per feature map.
The original model uses 6 default boxes per anchor point with different aspect ratios, any number below this could by testes.
Thirdly, we can use a smaller backbone, meaning one with fewer parameters.

\subsection{Multifocal data}
Of the proposed experiments, integrating multifocal data is probably the most ambitious and least defined.
As opposed to CNN based object detection, which has been an established field for many years, multifocal image processing with neural networks is, to the best of our knowledge, quite unexplored.
For this reason, specific experiments must be researched further.
As with\ \cite{gallardo_caballero_precise_2019}, it would be interesting to run the model using both a blurred and un-blurred training set to see if the results are different in a classification model.
More interesting would be to develop a model capable of training on sequences of images, but it remains to see if this is possible within the time constraints

\section{Schedule}
The following list of deadlines sketches the rough outline for the major activities for the coming months.
The first deadline is quite critical to allow for time for revisions as they become necessary. 

\vspace{2ex}
{\daymonth%
\begin{tabular}{>{\bfseries}ll}
  \formatdate{1}{3}{2020} & System is feature complete and all experiments are planned.\\[1ex]
  \formatdate{19}{4}{2020} & All Experiments are complete and data is gathered.\\[1ex]
  \formatdate{1}{6}{2020} & Thesis writeup is complete/ready for revision.
\end{tabular}
}

\vspace{2ex}
This thesis has attempted to determine how a CNN object detection can be used to further the pogress toward creating a system for automatic pollen counting.
We have uncovered oppertunities for unique contribitions to the existing knowledgebase that we hope a fully developed system may be able to deliver.
By the end of the project we hope to demonstrate that a computationally inexpensive model may be capable of performing the task of counting and classifying pollen grains.