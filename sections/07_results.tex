\chapter{Results}\label{cha:results}

This section will present and discuss the results from our experiments.
The modified SSD model presented in Section~\ref{sec:method-arch} is trained according to the training procedure given in Section~\ref{sec:method-exp-setup}.
Section~\ref{sec:results-baseline} presents the performance of the baseline model. 
The results from experiments pertaining to RQ1 and RQ2 are presented in Section~\ref{sec:results-simplification} and Section~\ref{sec:results-sharpness} respectively.
Unless specified otherwise, all performance metrics given in this section are measured on the test split of the dataset.
For measuring mAP we use an IoU of 0.5 as the acceptance threshold, and follow the interpolation method specified in Equation~\ref{eq:average-precision}

\section{Baseline model}\label{sec:results-baseline}
\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/method/baseline/loss2.pdf}
    \caption[Baseline training procedure]{%
Training procedure for the baseline model over 2000 iterations.
The iterations are given on the horizontal axis are represent one forward-backpropagation run with one mini-batch, while the mini-batch averaged loss is given in the vertical axis.
The raw total loss values are shown with semi-transparent green points.
The solid lines show the moving means for the two individual loss components.
The green line shows the confidence loss component, while the blue line shows the localization loss component.
The mini-batch with the lowest total loss is annotated with a green circle and occurs after the 1800th iteration with a loss of 0.496.
    }\label{fig:method-baseline-loss}
  \end{figure}

We start by presenting the results from training the model in its baseline configuration. 
The training procedure is given in Figure~\ref{fig:method-baseline-loss}.
The stochastic variation in the loss curve is characteristic for mini-batch training. 
We can also see that the confidence loss accounts for most of the total loss throughout the training process.
The components of the loss function (Equation~\ref{eq:loss}) are independent, meaning that their ratio does not indicate a difference in performance at the two different tasks.
We do however observe that the model performs better at localization than classification.
Logically this follows from the realization that the pollen grains are all distinct from the background, but are all quite similar.
The model is therefore able to discriminate between background and pollen with relative ease, but has a harder time classifying species.

\begin{figure}[htb]
    \centering
    \includegraphics[width=0.7\textwidth]{figs/method/baseline/detections_test.pdf}
    \caption[Detections by type by class for the baseline on the test split]{%
Breakdown of predictions by class.
For each label, the first column gives the number of ground truth labels in the test split.
The second column gives the number or predicted boxes for that class, broken down into true positive matches in green, and false negatives in red.
The difference between ground truths and true positives in the number of false negatives. 
    }\label{fig:method-baseline-detections}
  \end{figure}

The baseline model achieves a mAP of \textbf{95.2\%} and an \(F_1\) score of \textbf{86.8\%}.
Looking at predictions overall the recall is very high at 99.1\%, while precision is quite a bit lower at 77.2\%.
Figure~\ref{fig:method-baseline-detections} breaks down all the detections made, the main observation is that false positive predictions are the almost singular source of error.
37\% of false positives are miss-localizations, i.e.~an unlabeled entity is identified as a pollen grain.
These entities are either non-pollen particles or pollen grains from unlabeled species.
The remaining 63\% of false positives are miss-classifications, i.e.~the bounding box does correspond to a ground truth, but the class is incorrect.

\begin{figure}[htb]
  \centering
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/method/baseline/Snap-408.pdf}
  \end{subfigure}%
  \hspace*{0.04\textwidth}
  \begin{subfigure}[t]{0.4\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/method/baseline/Snap-028.pdf}
  \end{subfigure}
  \caption[Predictions showing TP overlapped by FP from different class]{Two predictions with GT in \textcolor{nicegreen}{green}, TP in \textcolor{red}{red}, and FP in \textcolor{nicepink}{pink}.
The labels give the first letter of the class and the prediction confidence in the range [0,100].
All TP labels are placed at the NW corner of the bounding box.
The FP labels are placed at the other three corners according to class.
In the left image, a FP box is predicted with a lower confidence than the TP\@.
In the right image, the FP corylus prediction has a higher confidence than the TP alnus prediction.}\label{fig:method-overlapping-predictions}
\end{figure}

Figure~\ref{fig:method-overlapping-predictions} shows two test samples containing overlapping true and false positive predictions.
This is due to the NMS filtering algorithm being run for each class independently, which causes double predictions in cases where predictions for multiple classes are produced for the same pollen grain.

A remedy for this was implemented and tested where NMS was applied to all detections after the initial per-class NMS\@.
The results from this were ambiguous; The precision did improve due to the decreased number of FP predictions, but the decrease in TP predictions caused a similar decrease in recall.
In Figure~\ref{fig:method-overlapping-predictions} (right) for example, the correct prediction would get filtered out.
In a model with improved classification confidence, this technique would be of benefit as it would improve overall precision without sacrificing recall.

\section{Model simplification}\label{sec:results-simplification}
The first two experiments aim to explore ways in which we can reduce the computational complexity of the model without sacrificing performance.
In the first experiment we have changed the feature extraction network with light weight alternatives.
In the second we have reduced the amount of default boxes by deactivating source feature maps.

\subsection{Feature extractor}
Table~\ref{tab:result-base-network} gives the results from training the network with different base networks.
Larger network could not be tested due to the memory limitations of the GPU used.
We see a clear decrease in performance when using smaller networks with all performance metrics monotonically decreasing with the size of the feature extractor.

Interestingly, the recall remains almost unchanged, meaning that the model's ability to localize pollen grains is largely maintained.
The performance degradation is therefore mostly due to weaker classification confidence.
A possible explanation for this is that localization as a task only requires the model to learn simple features relating to the general shape and color of a pollen grain, while classification involves more complex features that the smaller networks are not able to capture.




\begin{table}\centering
  \ra{1.3}
\caption[Performance by feature extraction network]{Summary of experimental results from testing various small feature extraction networks.
The localization score refers to the share of detection that correctly localize a GT, regardless of class.
For each network, the number of trainable parameters for both the feature extractor and whole model is given.}%
\label{tab:result-base-network}
\begin{tabular}{@{}lcrrrrcrr@{}}\toprule
  Network & \phantom{a} & \multicolumn{4}{c}{Performance} & \phantom{ab}&  \multicolumn{2}{c}{Parameters} \\
  \cmidrule{3-6} \cmidrule{8-9}
        &&  maP &  Precision &  Recall &  Loc. &&  Extractor   & Total \\
  \midrule                                                                     
     ResNet 34 && 95.2 &      82.7  &   99.1  & 95.0  &&  \num{8.2e6} & \num{1.2e7} \\
     ResNet 18 && 92.6 &      81.9  &   97.4  & 92.8  &&  \num{2.8e6} & \num{6.7e6} \\
  MobileNet V2 && 90.4 &      67.9  &   98.3  & 86.3  &&  \num{1.3e5} & \num{9.6e5} \\  
  \bottomrule
\end{tabular}
\end{table}

\begin{table}\centering
  \ra{1.1}
  \caption[Performance when deactivating source layers]{Summary of experimental results when deactivating various source layers.
  The source layers are ordered 1--6 with layer 1 being the most granular \(38\by38\) feature map.}%
  \label{tab:result-layer-deactivated}
\begin{tabular}{@{}llllllr@{}}\toprule
  \multicolumn{6}{c}{Source layer activation} & \\
  \cmidrule{1-6}
  1 & 2 & 3 & 4 & 5 & 6 &   mAP\\
  \midrule
  \ckm & \ckm & \ckm & \ckm & \ckm & \ckm & 95.2 \\
  \ckm & \ckm & \ckm & \ckm & \ckm &      & 95.8 \\
  \ckm & \ckm & \ckm & \ckm &      &      & 94.3 \\
  \ckm & \ckm & \ckm &      &      &      & \textbf{96.7} \\
  \ckm & \ckm &      &      &      &      & 95.0 \\
  \ckm &      &      &      &      &      & 96.1 \\
       & \ckm &      &      &      &      & 95.4 \\
  \bottomrule
\end{tabular}
\end{table}

%\section{Sharpness}\label{sec:results-sharpness}