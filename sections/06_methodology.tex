\chapter{Methodology}\label{cha:method}

\section{Data}\label{sec:dataset}
The results presented in Chapter~\ref{cha:results} are trained on data sourced from the Norwegian Asthma and Allergy Association, who have, since 1980, tracked the amount of air born pollen in Norway.
Pollen is collected with traps where air is continually sucked though a small slit and is redirected over an adhesive strip.
The strip is moved across the slit, exposing different sections throughout the day.
Pollen grains and other air born particulates adhere to the strip, which is then analyzed under a microscope.
Only pollen grains from a subset of species are actively tracked.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/Snap-057.png}
  \caption[Dataset example]{Example from the dataset with ground truth bounding boxes drawn. The image contains two classes: \textcolor{corylus}{corylus}, and \textcolor{alnus}{alnus}.}\label{fig:dataset-sample}
\end{figure}

Three microscope slides have been imaged using a digital optical microscope producing a set of 701 raster images with a size of \(1080\times 1920\) pixels and three channels; red, green, and blue.
The resolution of each image is \(\SI{0.183}{\micro\metre\per\pixel}\).
Each image has been labeled in collaboration with the experts to produce a valid and correct ground truths.
In total, three different species have been classified, namely \textit{poaceae}, \textit{corylus}, and \textit{alnus}, known in English as Grasses, Hazel, and Alder, respectively. A labeled example is given in Figure~\ref{fig:dataset-sample}. A summary of the dataset is given in Table~\ref{tab:dataset}.

\begin{table}[htb]
  \caption[Class distribution across the dataset]{Distribution of classes across the pollen dataset}\label{tab:dataset}
  \centering 
  \begin{tabular}{lrrr} \toprule
                      & Poaceae & corylus & alnus \\ \midrule
    Number of labels  & 5600    & 262     & 522 \\
    Proportion        & 87.7\%  & 4.1\%   & 8.2\% \\ \bottomrule
  \end{tabular}
\end{table}


Many of the images are taken from the same view point, but with the focus point set to different grains. The ground truth labeled are drawn for all present pollen grains, regardless of the how blurred they appear. This is done so that the dataset may be modified for the purpose of our analysis of sharpness and model performance in regards to \textit{RQ2}.

As opposed to more general object detection tasks, where there is a large variance in both the apparent size and shape of objects within an image, this dataset is much more regular. This is demonstrated in Figure~\ref{fig:aspect}, where we can see that the grains are mostly square in shape and between 100 and 150 pixels wide.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/aspect_ratio.pdf}
  \caption[Aspect ratios in the dataset]{The widths and heights of all ground truth boxes are plotted, longest against shortest. The lines denote the three aspect ratios used in the default boxes of the SSD model. Grains marked `Edge contact' are those in direct contact with the edge of the image and are most likely partially cropped out of frame. We can clearly see that the grains are quite regular in both shape and size}\label{fig:aspect}
\end{figure}

%\section{Architecture}\label{sec:architecture}

\section{Sharpness Measure}\label{sec:method-sharpness}
Analyzing how the sharpness of pollen grains affects detection performance requires an objective sharpness measure.
In this section we will detail how sharpness will be measured for the purpose of our analysis.
The measure is based on Fourier analysis and its performance has been tested on the training data.

\subsection{Fourier analysis}
Fourier analysis describes the general method of utilizing the Fourier transform to analyze the component frequencies present is some signal.
For the purpose of Fourier analysis we can interpret an image as a collection of signals, each of which describe the change in brightness value when traveling across the image in some direction.
Taking the Fourier transform of an image then produces a 2-dimensional matrix where the intensity of each element represents a the coefficient of a 2-dimensional sinusoid base component of the image.
Figure~\ref{fig:fourier-sinusoid} visualizes what the base components look like and demonstrates exactly what is encoded in the Fourier spectrum.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/fourier/fourier-sinusoid.pdf}
  \caption[Fourier transform of sinusoid]{The figure shows two 2D sinusoid wave forms and their Fourier transform. The active components in the transform have been enlarged so as to make them visible in print. Only a single pixel, and its reflection about the origin, is active.}\label{fig:fourier-sinusoid}
\end{figure}

Figure~\ref{fig:fourier-demo} shows the Fourier transform of various inputs.
The transforms are shifted, such that an element in the transform encodes a sinusoid component with a frequency proportional to its distance from the center and moving in the same direction as the vector it forms with the center. 
The directionality of the Fourier transform is demonstrated in the first two examples.
Squares decompose into a set on sinusoids, all moving is the same two directions.
The coefficient of each component is encoded in the intensity of the pixel.
In all the examples the lower frequency components dominate, which lights up the center region. The last two examples demonstrate how blurring an image eliminates the higher frequencies from the Fourier domain.



\begin{figure}[htbp]
  \centering
  \begin{subfigure}[b]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/square_original.png}
    \vspace*{0.02\textwidth}
  \end{subfigure}%
  \hspace*{0.02\textwidth}
  \begin{subfigure}[b]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/square_45deg_original.png}
    \vspace*{0.02\textwidth}
  \end{subfigure}%
  \hspace*{0.02\textwidth}
  \begin{subfigure}[b]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/peppers_original.png}
    \vspace*{0.02\textwidth}
  \end{subfigure}%
  \hspace*{0.02\textwidth}
  \begin{subfigure}[b]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/peppers_blur_original.png}
    \vspace*{0.02\textwidth}
  \end{subfigure}

  \begin{subfigure}[t]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/square_fourier.png}
  \end{subfigure}%
  \hspace*{0.02\textwidth}
  \begin{subfigure}[t]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/square_45deg_fourier.png}
  \end{subfigure}%
  \hspace*{0.02\textwidth}
  \begin{subfigure}[t]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/peppers_fourier.png}
  \end{subfigure}%
  \hspace*{0.02\textwidth}
  \begin{subfigure}[t]{0.2\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/fourier/peppers_blur_fourier.png}
  \end{subfigure}
  \caption[Demonstration of the Fourier transform]{
    Demonstration of the Fourier transform. The bottom row shows the center-shifted discrete Fourier transform of the corresponding image in the top row. The Fourier spectra mapped to grayscale values with a truncated linear amplitude mapping which is scaled to compensate for the otherwise dominating center component.}\label{fig:fourier-demo}
\end{figure}

Using the Fourier spectrum to measure sharpness follows from the realization that there is a strong relationship between the sharpness of an image in the spacial domain and the distribution of frequency components in the frequency domain.
Sharp features produce high frequencies while blur smooths out the changes in brightness, lowering the frequencies.
Figure~\ref{fig:fourier} shows three different pollen grains, captured with progressively more blur.
If we look at their corresponding Fourier spectra we can see that, as the perceived blur increases, the amount of high frequency components decrease.


\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.8\textwidth]{figs/fourier.png}
  \caption[Fourier spectrum]{Pollen grains and their corresponding centered Fourier spectrum. The Fourier spectra are log scaled so that the higher frequencies become visible.}\label{fig:fourier}
\end{figure}

\subsection{Measuring sharpness}
We must then decide how to encode this change in frequency distribution as a scalar sharpness measure.
\citeauthor{de2013image} propose a simple method which counts the number of components in the Fourier spectrum having a value above a certain threshold.
The operation is described in Equation (\ref{eq:sharp}).


\begin{equation}\label{eq:sharp}
  \begin{split}
    \mathbf{X} &= \mathcal{F}(a),\quad a\in \mathbb{Z}^{M\times N}\\
    T_H &= \sum_{x\in\mathbf{X}}[x\ge\mu],\quad \mu=\frac{\max \mathbf{X^{\mathrm{abs}}}}{1000}\\
    S &= \dfrac{T_H}{M\times N}
  \end{split}
\end{equation}

Here \(\mathcal{F}\) is the discrete the Fourier transform, operating on a input image \(a\), and \(\mathbf{X}\) only contains the magnitude of the Fourier transform.
The scaling factor of the threshold value, \(\mu \), was found to produce good results on our data, without modification. \(S\) is the sharpness measure.

Validating the sharpness measure is an important task.
The weight of any argument made based on analysis using this measure is predicated on its soundness.
There are many different approaches to this, we have chosen to compare the objective measure with subjective measurements of perceived sharpness on a subset of the training data.

\subsection{Evaluating the sharpness measure}
The basis of our evaluation is a new dataset created from a small random subset of the training examples.
Each ground truth is then given sharpness scores based on perceived sharpness and the objective measure.
Determining perceived sharpness of a pollen grain with high fidelity was found to be highly subjective and non-reproducible with repeated independent scoring, so a simple classification was instead performed.
Images where separated into three classes representing perceived sharpness.
Figure~\ref{fig:fourier} gives examples of the classes with image (a) being the sharpest (class 3) and (c) being the blurriest (class 1).
A total of 389 pollen grains where evaluated, the distribution of their classes is given in Table~\ref{tab:sharpness}

\begin{table}[htbp]
  \caption[Sharpness dataset distribution]{Distribution of classes across the sharpness evaluation dataset.}\label{tab:sharpness}
  \centering 
  \begin{tabular}{lrrr} \toprule
    & blurry [1] & partly [2] & sharp [3]  \\ \midrule
    Number of labels & 123 & 105 & 125 \\ \bottomrule
  \end{tabular}
\end{table}

\begin{figure}[htbp]
  \centering
  \begin{subfigure}[t]{0.49\textwidth}
    \centering
    \includegraphics[width=\textwidth]{figs/box.pdf}
    \caption{Box plot}\label{fig:sharpness-box}
\end{subfigure}%
\hfill
\begin{subfigure}[t]{0.48\textwidth}
  \centering
  \includegraphics[width=\textwidth]{figs/qden.pdf}
  \caption{Density plot}\label{fig:sharpness-qden}
\end{subfigure}
  \caption[Sharpness measure separability]{The sharpness measure grouped by class label. There is a large overlap between adjacent classes, but the IQRs in (a) are clearly separated. The correlation between perceived and measured sharpness is also clear.}\label{fig:sharpness}
\end{figure}

Looking at Figure~\ref{fig:sharpness} we can see a clear correlation between the mean of the distribution, and the perceived sharpness.
The overlap is to be expected, given the mapping from a continuous predictor onto a categorical label.
To further evaluate the performance of the sharpness measure, we construct a very simple statistical model, in the form of a decision tree, and test its ability to differentiate between the classes.
The model achieved a test accuracy of 84.1\%. 

In Figure~\ref{fig:sharpness-all} we have computed the sharpness of every ground truth in the dataset in a density plot showing the distribution.
As expected, most of the ground truths fall into the sharpest category, but there is a spread, allowing us to analyze the relationship between sharpness and model performance.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/sharpness_all.pdf}
  \caption[Distribution of sharpness across entire dataset]{Density plot showing the sharpness of all pollen grains in the dataset. The shaded sections indicate the distribution of sharpness classes, if the dataset is classified with the evaluation model. As expected, most grains fall into the sharpest category.}\label{fig:sharpness-all}
\end{figure}

\section{Architecture}

The model we have chosen to implement is the Single Shot Multibox Detector, a brief explanation of which was given in Section~\ref{sec:ssd}.
This section gives a more thorough explanation of the model and its implementation with a focus on the changes that have been made from the original.
Most of these changes are motivated by more recent research featured in younger models.
The reasoning behind choosing a now relatively old model framework is mostly based on its architecture, which lends itself to alteration and simplification.


Our implementation is based on an implementation by NVINIA\footnote{Released under the Apache 2.0 License, \href{https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD}{URL:\\ } \url{https://github.com/NVIDIA/DeepLearningExamples/tree/master/PyTorch/Detection/SSD}}, the basic structure is shown in Figure~\ref{fig:model}.
For the implementation we divide the model into four parts.
First, we have the feature extraction network, which feeds into an \textit{auxiliary structure} which gradually steps down the dimensions of the feature map.
Finally we have two detection structures, one for class confidences, and one for bounding box regression.

\subsection{Model}
The feature extractor in the original model is the VGG-16 network~\parencite{simonyan2015deep}, but as is pointed out by the authors, this choice is arbitrary and other networks can also be used.
In our implementation we use ResNet-34~\parencite{he2015deep}.
The main reason for this being its superior performance over VGG, and the ease of which it integrates with the auxiliary structure.
With VGG, alterations are required to transform the latter layers from FC to CNN, while with ResNet we can just truncate the network at the appropriate layer to produce a feature map with the appropriate dimensions.
PyTorch offers canonical implementations of the most popular pre-trained models as part of their API\@.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.9\textwidth]{figs/model.pdf}
  \caption[Model architecture overview]{Overview of the general structure of the model. The figure is inspired by the one given in the original paper, but has been modified to reflect our alterations.
The horizontal lines denote the detection structure comprised of \(3\times 3\) convolutions for class confidences and bounding box regressions.
Here \textsf{C} is the number of classes including one for background}\label{fig:model}
\end{figure}

The auxiliary structure is implemented as a sequence of blocks where the output of each block is a feature map which is consumed by the detection structure.
Each block contains multiple convolutional layers, as specified in Figure~\ref{fig:model}.
PyTorch offers different ways to group multiple layers together so that they act as a single unit.
This makes the forward pass simple to define.

The detection structure is implemented as a sequence of distinct layers, each of which runs over one of the intermediate feature maps from the auxiliary structure.
Each localizing layer has four filters per default box, one for each of the regression parameters.
Similarly, the class prediction layers have \(C\) filters per box, one for each class with an additional layer for the background.

Because of the dynamic graph, the actual structure of the network is defined at runtime by defining a function for the forward pass.
The input is first passed through the feature extractor, it is then passed through each block of the auxiliary structure, saving each feature map.
The final output is then generated by running each feature map through the corresponding detection layer.
All the outputs from the localization and classification layers are concatenated, producing two output tensors.
The dimensions are \(\left(B,4,D\right) \) and \(\left(B,C,D\right) \) for the localization and classification outputs, respectively.
Here \(B\) is the batch size and \(D\) is the number of default boxes.


\subsection{Training objective}
Because of the default boxes, each ground truth box needs to be matched to one or more default boxes so that we have a target for the loss calculation.
SSD has a generous matching strategy where ground truths and default boxes are matched if their IoU is above a certain threshold. Figure~\ref{fig:priors} shows the result of the matching strategy. Every red box in the image is a default box that has been matched with a ground truth, shown in green. For the purposes of training, these are the default boxes that the loss function expects the model to has correctly labeled and regressed.

\begin{figure}[htb]
  \centering
  \includegraphics[width=0.5\textwidth]{figs/priors_matching}
  \caption[Default box matching]{Visualization of the result of the matching procedure.
In \textcolor{red}{red} are all the default boxes that are matched to the ground truths, i.e.~those that have an IoU \( \geq 0.4 \) with a ground truth box, in \textcolor{nicegreen}{green}.}\label{fig:priors}
\end{figure}

This ensures that there are many positive examples for each ground truth.
The loss function is calculated as follows:

\[L(x,c,l,g)=\frac{1}{N}\left( L_{conf}(x,c) + \alpha L_{loc}(x,l,g)\right) \]

where \( x=\{1,0\} \) is a binary mask denoting a match between a default box and ground truth, \( N \) is the number of positive examples, \( c \) is class confidences, \( l \) is predicted boxes, and \( g \) is regressed ground truth boxes.  \( L_{loc} \) is the summed Smooth L1 loss over all positive matched bounding boxes, while \( L_{conf} \) is the SoftMax loss over multiple class confidences, both are standard loss functions provided by PyTorch.

A problem that this and many similar models face is the gross imbalance between positive and negative training examples.
Out of the 8732 default boxes only a small subset can be matched to to a ground truth.
For the localization loss this has no impact as only the matched boxes are counted, but for the confidence, the overwhelming majority of predictions are negative matches.
To counteract this imbalance SSD uses \textit{hard negative mining} to balance out the negative and positive examples.
The confidence losses for all negatives are sorted and only the negative examples with the highest loss are chosen such that the ratio between positives and negatives is 1:3.

