\chapter{Related Work}\label{cha:related}
Object detection using CNNs is a fairly new area of study, and as such, its application in the domain of pollen counting is lacking in literature.
Examining related work therefore requires us to widen our field of view and explore the ways in which similar methods have been used to solve similar problems.
We will do this in to parts; Firstly, we will examin the various object detection frameworks and their use within microscopy.
Secondly, we will detail the various methods that have been employed in relation to our specific domain.

\section{Convolutional Neural Networks}\label{sec:rel-cnn}
Before CNNs, the task of classifying images was usually highly dependent on the problem domain.
Careful feature engineering was used to extract a set of parameters which were then classified using a statistical model.
A CNN fundamentally changes this landscape by removing all pre-processing.
Over the last 10 years, CNNs have risen to prominence as the state of the art in image processing.
Raw images are classified directly, with little consideration of the specific domain.
The trade-off is the near insatiable thirst these models have for labelled training data required to train them.

\subsection{Object detection}
With the quality of image classifiers rising, focus has been given to the more complex task of object detection, where the model must identify the location of objects within an image, as well as their class.
The work on this problem was kickstarted by~\cite{girshick_rich_2014}, who proposed R-CNN, an algorithm which operates in two distinct steps.
First by producing \textit{regions of interest} within the image, and then running those regions through a classifier.
They later proposed Fast R-CNN and Faster R-CNN which improved the learning and inference time, as well as the robustness, of the original model.

R-CNN has three main modules.
First, bounding boxes are proposed using selective search, an algorithm where different similarity measures are first used to segment the image before then being recursively combined together to generate larger \textit{regions of interest}.
Each region is then resized and fed into the second module, a CNN which produces a feature embedding which is then classified using a SVM, the third stage.
A major computational bottleneck was having to process each region proposal independently through the second and third stage.

Fast R-CNN removes the second and third stages, and replaces them with a new CNN which takes as input the whole image and the region proposals and generated predictions for all regions in one go.
This new second stage also predicts offsets for the proposed boxes, allowing it to refine the proposals from the first stage.

Faster R-CNN replaces the first stage with a Region Proposal Network (RPN), a fully convolutional deep neural network which produces a fixed number of bounding boxes together with an `objectness' score for each box.
The RPN introduces the concept of anchors, which are points in the image used to regress bounding boxes.
After a set of convolutional layers, we are left with a \(n\times n\) feature map.
If we slide a window over this feature map we can predict the dimensions of one or more bounding boxes anchored at the centre of the window.
Given that a convolution \textit{is} a sliding window operation we only need to choose an appropriately sized window and how many boxes we want to predict and set the kernel size and output channels thereafter.
Using a RPN allows of training of both stages of the detector, significantly increasing the performance.
Following the release of Faster R-CNN, the amount of research attempting to automate various object detection tasks has increased. 

In many domains there is usually a positive correlation between the cost of data and its quality.
Often times methods that are proposed therefore become prohibitively expensive because they make use of higher quality data.
CNN based methods have however shown that high quality models can be created using lower quality data.~\cite{el_melegy_automatic_2019} gives a good example of this.
A Faster R-CNN method is proposed for detecting tuberculosis bacilli in LM slides.
The model they propose is able to outperform all previous traditional models, many of which use higher quality imaging methods.
The type of images the model uses are very difficult to diagnose manually, but are by far the most available in the field.
The model also solves an issue present in most of the previous work, namely how to actually automate the process of diagnosis.
Previous work is based on pre-segmented images which are then classified, which requires a human expert.
This is common for a lot of research where a problem involves localization. 

\section{Single Stage Detectors}\label{sec:ssd}
Common to the R-CNN family of methods is the use of two separate stages, one for identifying regions of interest in an image, and one for classifying objects in those regions.
This adds considerable complexity in that both systems require separate training and tuning.
These methods have been successfully utilized in many domains, including the only published attempt at pollen grain detection.
The inference speed does however remain prohibitively slow for many tasks that require real-time performance.
We can ovserve an obvious trend in the evolution of the two stage systems where stages are merged or replaced by CNN\@.
This trend continues to its logical conclusion with the development of the single stage detector.

This class of model features only a single CNN, responsible for both localizing and classifying objects.
These models can be trained in one pass, and feature inference speeds orders of magnitude faster than Faster R-CNN\@.
One of the first methods was the Single Shot Multibox Detector (SSD), proposed by\ \cite{liu_ssd_2016}, which is the model that has been implemented in Section~\ref{cha:tech}.
It was one of the first models to demonstrate that a single-stage model could vastly improve inference speeds without compromising accuracy. 

The architecture resembles the RPN from Faster R-CNN\@.
Like a RPN, SSD predicts bounding box offsets for a fixed number of \textit{default} bounding boxes.
These boxes are centered with anchors, and the height and width predictions made by the model are regressions from a set of default boxes.
SSD makes predictions from multiple feature layers, and the default boxes are scaled up as the depth of the feature maps increases, allowing the network to predict objects at different scales in the image.
For the object classification the network replaces the objectness score in the RPN with class confidence scores for all classes.
The training objective for SSD is created by matching default boxes and ground truth boxes based on their IoU score.
The model is trained by combining two separate loss values, one for the class confidences, and one for the bounding box regressions.
Most default boxes are not matched to any ground truth box.
To balance out the ratio of negative and positive examples in the training objective, only the negative examples with the highest loss values are included in the final loss value.

As of writing, there are no published attempts of using a single stage detector to count pollen grains, but there are examples in similar domains.~\cite{liu_brain_2018} uses a SSD model to detect brain slices used in an automatic sample preparation system.
The model was chosen for its speed and accuracy, both important in real-time detection.
The model was also simplified because the range of scales to be detected was narrower than what the standard SSD model has implemented.
This simplification could also prove relevant to a pollen detection task where grains are similar in both size and shape.
Because of the structure of the SSD model, this simplification is achieved just by removing layers from the model, thus removing predictions from those scales.
Results show that the simplified model increased both accuracy and speed over the original.

You Only Look Once (YOLO) is a very popular framework that has been applied to many different areas.
As with R-CNN it has been released in many versions with iterative improvements.
It is similar to SSD in that it also predicts box offsets and class scores for a fixed number of bounding boxes.
Specifically, YOLO divides an input image into a grid and then predicts box offsets for a fixed number of bounding boxes centred in each grid square (but not regressed from default boxes), and class confidences for each grid square.
Similarly, YOLO also uses transfer learning to reduce training time and can be configured to use most any model as its feature extractor.
The later versions of YOLO also feature multiple extraction layers which improves predictions for smaller objects, which was one of the major weaknesses of the initial version.

Recently, multiple papers using YOLO models have been published showing promising results in areas similar to palynology.~\cite{chibuta_real_time_2020} uses a modified version of the third iteration of YOLO to screen blood smears for malaria.
Diagnosing malaria is very costly because it requires manual analysis of blood samples.
Similar to Tuberculosis, the areas with the highest prevalence of the disease, are those where the primary screening technique uses light field microscopy.
The presented model uses a smaller feature extractor and fewer extraction layers to optimize for speed on basic hardware.
The model has \( \simeq 99\% \) fewer parameters than the standard YOLOv3 implementation and still performs at the same level as both human experts, and its unoptimized equivalent.

Another area of study that could be of interest to this thesis is blood cell counting.
A complete blood count is a test that is often requested when evaluating general health and involves a manual count of blood cells within a sample.
The current standard process requires human expert analysis and is prone to error.~\cite{islam_machine_2019} proposes a YOLO model which is able to accurately localize and classify blood cells using standard LM images of a blood smear.
The YOLO model has not been modified apart from changing the number of classes to 3 (Red, White, and Platelets).
They do however change the inference routine to optimize the count for each of the three cell types.
As with the aforementioned RPN, YOLO predicts an objectness score for each bounding box, and normally considers a box to be a positive match if the score exceeds a threshold.~\citeauthor{islam_machine_2019} show that, rather than using only one threshold value for all classes, a higher overall accuracy can be reached by filtering boxes for each class independently with different thresholds.
This points towards a larger issue of choosing both the algorithms that are used to filter the predictions made by these models and their hyperparameters.
The difference between how many raw predictions a model makes, and the number of objects an image contains is usually many orders of magnitude.
The method used to filter the predictions is therefore crucial to the performance of the model, and also highly dependent on the domain.

\section{Automated Pollen Detection}\label{sec:rel-pollen}
There have been many attempts at automated pollen \textit{classification} over the last three decades.
These have been nicely summarized in~\cite{sevillano_improving_2018}.
Most are statistical classifiers using selected features from pollen images.
The earlier attempts can be grouped into three categories.
The first focuses on morphological features such as shape, size and symmetry.
The second type uses the texture of the grain surface as the discriminating feature.
The last group uses a hybrid approach which combines morphological and texture features.
These methods have been able to successfully classify pollen to a degree comparable to human experts, but all rely on careful feature engineering.
Of the earlier methods, the most successful utilize images taken through SEM, which is a much more expensive imaging technique than standard LM imaging.

\subsection{Classical methods}
As a precursor for the newer systems, it is pertinent to cover the earlier attempts at solving the problem of automated pollen counting.
The first attempt, by~\cite{langford_computerized_1990}, used grey scale SEM images of the surface texture on pollen grains. 16 feature measures, based on a Grey-tone spatial dependence analysis, were then produced and classified using Linear Discriminant Analysis.
This technique was successful, but required manual analysis for each class, making it difficult to apply to new datasets or other taxa of pollen.

Other attempts were made over the next decade, some using morphological features instead of surface texture, but they follow the same basic procedure of feature engineering followed by a statistical classifier.
The next major contribution was made in~\cite{li_pollen_1999}, in which very high accuracy was achieved using LM images.
The major disadvantage of LM imaging was the shallow depth of field causing only portions of the pollen grains to be in focus.
This reduction of image quality caused a loss in accuracy of the LDA based methods.
The new method exchanged the classifier with a Multi-Layer Perceptron, and achieved higher scores than previous methods using a simpler set of feature measures.
The main limitation was the lack of processing power at the time which meant that the method could not scale to larger sets of images.

As computational power rose, there were also attempts made at localizing grains.\ \cite{france_new_2000} presented a hybrid solution featuring a multi stage system of both classical and neural methods.
The localization was handled by a K-means classifier coupled with a shape and size filter, producing segments of the image likely to contain pollen.
A trained classifier is then used to classify the grains.
The results were promising, but the system was also very limited.
Firstly it was very sensitive to focus, and could only work with grains perfectly within the depth of field.
Secondly the segmentation algorithm only worked on sparse images with enough space between grains, and could not separate grains if they were too close.
These same issues also create problems for modern systems, albeit not to the same extent.

Convolution has been an important tool in image processing since before CNNs gained traction.
Using hand crafted filters, many basic features such as edges, can be extracted from an image.
This technique was employed by\ \cite{DaoodICPR16b} with good results.
The system used a SVM for the final classification, but again demonstrated the viability of using convolutional filters as feature extractors.
This system in some ways bridges the gap towards the CNN models, but crucially lacks the ability the learn which features should be extracted.
This is the fundamental deficiency common to all the classical methods, they rely on human expert knowledge to adapt each method for use in the specific domain.

\subsection{CNN methods}
CNNs have all but taken over as the standard in image classification, and this is also the case in pollen detection.
Recently all the proposed methods involve a deep convolutional neural network as the main feature extractor.
Comparing the different models that have been presented is however very challenging.
Most use self-collected datasets which vary in size, both with respect to the number of classes and examples per class.
There is also an inherent difference in the difficulty of separating instances within any given dataset because some types of pollen are much more similar than others.
Meaningful comparisons are therefore difficult to make.
A performance comparison is however not necessary for this thesis, seeing as they are all classifiers, with no direct link to the requirements of our system.

{\cite{daood_pollen_2016}} presented a CNN method that was used on both a LM and SEM dataset and compared the results with many of the classic statistical methods, showing the clear benefit of using a CNN architecture.
A second network was also implemented which used transfer learning to further improve accuracy.
Data augmentation was used to combat the small size of the dataset, something that has been a pervading issue for most of the presented solutions.
Both transfer learning and data augmentation is featured in most of the papers after this.
The methods also showed that, although a higher accuracy was obtained on the SEM data, the models were fully capable of achieving good results using LM images.

{\cite{sevillano_improving_2018}} later gave more evidence for the superiority of CNN methods by applying three different convolutional models on a publicly available dataset {POLENE23E}, which at this point only had been classified using classic methods.
All three models used a CNN as the feature extractor and they all performed well, doubling the precision over the state of the art.
The results also show that there are only insignificant improvements when using a linear discriminant classifier on top of the CNN\@.

Common to all the methods we have mentioned is that the data they rely on is LM images of a single grain.
This ignores two important factors, Firstly and most obviously is than none of these models can be used directly to count pollen grains, only to classify pre-segmented images of singular grains.
This also leads into the second important factor of dealing with images of pollen grains at different focal planes.

Grains of pollen on a slide are not distributed across only one focal plane, grains are distributed across all three axis and because of the narrow depth of field of the microscope are only visible clearly when focused on.
The depth of field is in fact so shallow that only parts of the surface ornamentation appear clear.
For the models we have covered, this means that they potentially are missing out on features because they only rely on a image of a single focal plane.
In the case of classification, one possible remedy for this is to use a stack of images taken at different focal planes and process the stack as one unit.\ \cite{DaoodAndRibeiro_2018} uses this approach.
The model they propose takes as input a sequence of 10 images taken of pollen grains spanning the whole grain, and classifies it using a network which combines a \textit{recurrent} neural network and a convolutional neural network.
A recurrent neural network is a special type of network used to process sequences of information, e.g.\ signals, text, and in this case, 3D image stacks.
The results are promising, but the dataset is small, and with only one example, little can be said as to the potential efficacy of this technique.

The last model we will look at is also the only published work attempting to use a CNN to detect pollen grains in slide images.\ \cite{gallardo_caballero_precise_2019} Uses Faster R-CNN to detect pollen grains of various types within un altered LM images of pollen grains.
The model does not classify the grains it localizes.
They report very high values for both precicion and recall, but use a slightly modified definition of IoU when calculating these values.
They have released the dataset, which was used to train the prototype model, described in Section~\ref{cha:tech}

The dataset they use was created by filming the slide whilst changing the focus accross the pollen grains.
Based on a keyframe, 10 frames where extracted before and after the keyframe, creating a 21-frame sequence of focal planes.
From this, two datasets where created.
In the first one, the pollen grain where labled in the frame there they appered sharpest, and ignored in all other images.
In the second dataset, the grains where labled in all images throughout the sequence.
Only the lables from the first dataset where released.

The performance of the model was claculated by stacking the individual predictions from all focal planes together, and then performing the filtering step.
This workes well because the model makes no class predictions and would have to be adapted to be able to handle multi class predictions.
The definition of IoU is also changed so that it is capped to 1 if one box encloses the other.
The effect this has on the values for recall and presicion is not declared.

Two trials where run to compare the performance obtained from the two datasets, and values for recall and precision were high for both (above .98), with the non-blurred trial having slightly higher presicion.
The authors conclude that there does not seem to be any benefit of including blurred examples in the training set.
The results are hard to interpret, given the changed definition of IoU, but are very promising.

In this section we have detailed both how CNNs are used to solve object detection problems, and how the field of automated pollen grain analysis has evolved from using highly specialized hand crafted feature extractors, to being dominated by generalized CNN based frameworks.
Of most significance to this research are the modifications that have been successfully made to CNN models used in similar fields to pollen grain counting, as well as the different techniques that have been used to make use of multifocal data.